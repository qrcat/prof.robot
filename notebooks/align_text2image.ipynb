{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "if 'notebooks' not in os.listdir(os.getcwd()):\n",
    "    os.chdir('../') #changing directories so that output/gsplat_full etc. exists\n",
    "\n",
    "from collision.utils import DummyCam, ImageDemoDataset, generate_camera, put_pose_into_mujoco, update_reconstruction_dict, get_normalized_function\n",
    "from utils.mujoco_utils import compute_camera_extrinsic_matrix\n",
    "from scene.cameras import Camera_Pose\n",
    "from collision.chain_utils import build_chain_relation_map\n",
    "from collision.network import SingleNetwork, HyperNetwork\n",
    "from contextlib import redirect_stdout\n",
    "from video_api import initialize_gaussians\n",
    "\n",
    "import cv2\n",
    "from gaussian_renderer import render\n",
    "import sys \n",
    "import torch \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import mujoco\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from IPython.display import display, clear_output\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_preprocess = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "clip_model.to(device)\n",
    "for param in clip_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['']\n",
    "gaussians, background_color, sample_cameras, kinematic_chain = initialize_gaussians(model_path='output/shadow_hand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mujoco\n",
    "model = mujoco.MjModel.from_xml_path(os.path.join(gaussians.model_path, 'robot_xml', 'scene.xml'))\n",
    "data = mujoco.MjData(model)\n",
    "\n",
    "mujoco.mj_resetData(model, data)\n",
    "\n",
    "\n",
    "def sample_collision_pose():\n",
    "    pose = np.zeros_like(model.jnt_range[:, 0])\n",
    "    put_pose_into_mujoco(model, data, pose)\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set camera\n",
    "dummy_cams = [\n",
    "    DummyCam(0, -90.0, 0.4, lookat=[0.15,  0.0, 0]),\n",
    "    # DummyCam(0, -90.0, 0.4, lookat=[0.15,  0.1, 0]),\n",
    "    # DummyCam(0, -90.0, 0.4, lookat=[0.15, -0.03, 0]),\n",
    "    # DummyCam(0, -60.0, 0.4, lookat=[0.15,  0.0, 0]),\n",
    "]\n",
    "cams = [generate_camera(dummy_cam) for dummy_cam in dummy_cams]\n",
    "\n",
    "renderer = mujoco.Renderer(model, 480, 480)\n",
    "renderer.update_scene(data, camera=cams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mujoco.mj_resetData(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = sample_collision_pose()\n",
    "\n",
    "renderer.update_scene(data, camera=cams[0])\n",
    "pixels = renderer.render()\n",
    "image = Image.fromarray(pixels)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(\"output/shadow_hand\")\n",
    "\n",
    "relation_map, chain = build_chain_relation_map((output_path / \"robot_xml/scene.xml\").as_posix())\n",
    "sdf_model = HyperNetwork(chain.n_joints, relation_map)\n",
    "state_dict = torch.load(output_path / 'sdf_net.ckpt', weights_only=True)\n",
    "sdf_model.load_state_dict(state_dict)\n",
    "for parameters in sdf_model.parameters():\n",
    "    parameters.requires_grad_(False)\n",
    "sdf_model.cuda()\n",
    "del state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_fun, unnorm_fun = get_normalized_function(*kinematic_chain.get_joint_limits())\n",
    "\n",
    "init_params = [0.0] * 24\n",
    "joint_angles = torch.nn.Parameter(\n",
    "    torch.tensor(init_params, dtype=torch.float32, device='cuda')\n",
    ")\n",
    "optimizer = torch.optim.Adam([joint_angles], lr=0.02)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.95)\n",
    "\n",
    "background_tuple = (255, 255, 255)\n",
    "bg_color_t = torch.tensor(background_tuple).float().cuda() / 255.0\n",
    "\n",
    "cam_list = []\n",
    "\n",
    "for dummy_cam in dummy_cams:\n",
    "    camera_extrinsic_matrix = compute_camera_extrinsic_matrix(dummy_cam)\n",
    "    cam_list.append(\n",
    "        Camera_Pose(torch.tensor(camera_extrinsic_matrix).clone().detach().float().cuda(), 0.78, 0.78, 480, 480, joint_pose=norm_fun(joint_angles), zero_init=True).cuda()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = \"A black robotic hand do OK gestures with white background\"\n",
    "with torch.no_grad():\n",
    "    # text_input_t = clip.tokenize([text_input]).to(device)\n",
    "    # embedding_input = clip_model.encode_text(text_input_t)\n",
    "    inputs = clip_preprocess(text=[text_input], return_tensors=\"pt\", padding=False)\n",
    "    for key in inputs.keys():\n",
    "        inputs[key] = inputs[key].to(device)\n",
    "\n",
    "    embedding_input = clip_model.get_text_features(**inputs)\n",
    "print(embedding_input.shape, embedding_input.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iteration = 400\n",
    "\n",
    "topil = transforms.ToPILImage()\n",
    "\n",
    "camera_pkg = cycle(cam_list)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n",
    "\n",
    "with torch.no_grad():\n",
    "    joint_angles[:] = 0.0\n",
    "\n",
    "tbar = trange(max_iteration, leave=True)\n",
    "for iteration in tbar:\n",
    "    camera = next(camera_pkg)\n",
    "    \n",
    "    camera.joint_pose = norm_fun(joint_angles)\n",
    "\n",
    "    output_pkg = render(camera, gaussians, bg_color_t)\n",
    "    image_tensor = output_pkg['render'].clamp(0, 1)\n",
    "    depth_tensor = output_pkg['depth']\n",
    "\n",
    "    image_embedding = clip_model.get_image_features(preprocess(image_tensor)[None])\n",
    "\n",
    "    loss = -torch.matmul(image_embedding, embedding_input.T.detach())\n",
    "    loss = loss / len(cam_list)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    if iteration % len(cam_list) == 0:\n",
    "        torch.nn.utils.clip_grad_norm_(joint_angles, 10.0, norm_type=2)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # scheduler.step()\n",
    "        clear_output(wait=True)\n",
    "        display(topil(image_tensor))\n",
    "    \n",
    "    tbar.set_postfix({\n",
    "        \"loss\": format(loss.item(), \".3f\"),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "put_pose_into_mujoco(model, data, joint_angles.detach().cpu().numpy())\n",
    "print(data.ncon)\n",
    "renderer.update_scene(data, camera=cams[0])\n",
    "pixels = renderer.render()\n",
    "image = Image.fromarray(pixels)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iteration = 100\n",
    "\n",
    "with torch.no_grad():\n",
    "    joint_angles[:] = 0.0\n",
    "\n",
    "camera_pkg = cycle(cam_list)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n",
    "\n",
    "tbar = trange(max_iteration, leave=True)\n",
    "for iteration in tbar:\n",
    "    camera = next(camera_pkg)\n",
    "    \n",
    "    camera.joint_pose = norm_fun(joint_angles)\n",
    "\n",
    "    output_pkg = render(camera, gaussians, bg_color_t)\n",
    "    image_tensor = output_pkg['render'].clamp(0, 1)\n",
    "    depth_tensor = output_pkg['depth']\n",
    "\n",
    "    image_embedding = clip_model.get_image_features(preprocess(image_tensor)[None])\n",
    "\n",
    "    loss = -torch.matmul(image_embedding, embedding_input.T.detach())\n",
    "    \n",
    "    sdf , s = sdf_model(joint_angles[None])\n",
    "\n",
    "    if sdf > -0.1:\n",
    "        loss = loss + sdf * 5 \n",
    "    loss = loss / len(cam_list)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    if iteration % len(cam_list) == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # scheduler.step()\n",
    "        clear_output(wait=True)\n",
    "        display(topil(image_tensor))\n",
    "    \n",
    "    tbar.set_postfix({\n",
    "        \"loss\": format(loss.item(), \".3f\"),\n",
    "        \"sdf\": format(sdf.item(), \".3f\"),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "put_pose_into_mujoco(model, data, joint_angles.detach().cpu().numpy())\n",
    "print(data.ncon)\n",
    "renderer.update_scene(data, camera=cams[0])\n",
    "pixels = renderer.render()\n",
    "image = Image.fromarray(pixels)\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
